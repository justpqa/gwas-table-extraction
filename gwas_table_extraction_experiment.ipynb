{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8e006c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justpqa/advpai/venv/lib/python3.10/site-packages/pypdf/_crypt_providers/_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from cryptography.hazmat.primitives.ciphers.algorithms in 48.0.0.\n",
      "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n",
      "/Users/justpqa/advpai/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "import json\n",
    "# import xml.etree.ElementTree as ET\n",
    "# from rich.tree import Tree\n",
    "# from rich import print as rprint\n",
    "import io\n",
    "from typing import List, Union, Tuple, Dict\n",
    "from collections.abc import Iterable\n",
    "from tqdm import tqdm\n",
    "# import pdfplumber\n",
    "# import fitz \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "# import xmltodict\n",
    "import re\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "from difflib import SequenceMatcher\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions, TableFormerMode\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.document import DocumentStream\n",
    "from docling.pipeline.vlm_pipeline import VlmPipeline\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, BitsAndBytesConfig, LogitsProcessor, LogitsProcessorList\n",
    "# device = torch.device(\"mps\")\n",
    "from table_link_to_excel import _curl_get_text, _extract_pmc_info, _try_pmc_direct_table_download, _flatten_columns, sanitize_sheet_name, fetch_html, fetch_pmc_fulltext_xml, pick_table, table_to_dataframe, _clean_text\n",
    "from bs4 import BeautifulSoup\n",
    "from gwas_column_matching_engine import GWASColumnMatchingEngine\n",
    "from gwas_table_extraction import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cd6002",
   "metadata": {},
   "source": [
    "Try to look at the pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfd7240",
   "metadata": {},
   "source": [
    "Try to look at the website of the paper instead"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc6bc82",
   "metadata": {},
   "source": [
    "Try to use Docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870cbbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to know the number of col of a table to make sure we try the right orientation with docling\n",
    "def extract_tables_num_col_lst_from_pmc(pmcid):\n",
    "    url = f\"https://www.ncbi.nlm.nih.gov/research/bionlp/RESTful/pmcoa.cgi/BioC_json/{pmcid}/unicode\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    tables_num_col_lst = []\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        for d in data:\n",
    "            doc = d[\"documents\"]\n",
    "            for p in doc:\n",
    "                passage = p[\"passages\"]\n",
    "                for item in passage:\n",
    "                    if item.get(\"infons\", \"\").get(\"type\", \"\").lower() == \"table\" and \"text\" in item:\n",
    "                        table_str = item[\"text\"]\n",
    "                        num_col = 0\n",
    "                        for row in table_str.split(\"\\t \\t\"):\n",
    "                            row_lst = row.split(\"\\t\")\n",
    "                            num_col = max(num_col, len(row_lst))\n",
    "                        tables_num_col_lst.append(num_col)\n",
    "        print(f\"Successfully retrieve number of columns\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve number of columns: {response.status_code}\")\n",
    "        \n",
    "    return tables_num_col_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085e4822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning value with tag\n",
    "def clean_cell(val):\n",
    "    tag_pattern = r'\\s[a-z]$'\n",
    "    if isinstance(val, str):\n",
    "        return re.sub(tag_pattern, '', val)\n",
    "    return val\n",
    "\n",
    "def clean_headers(df):\n",
    "    tag_pattern = r'\\s[a-z]$'\n",
    "    new_cols = []\n",
    "    seen = {}\n",
    "    for col in df.columns:\n",
    "        if pd.isna(col):\n",
    "            new_cols.append(\"\")\n",
    "        else:\n",
    "            # 1. Apply the regex to the name string\n",
    "            clean_name = re.sub(tag_pattern, '', str(col))\n",
    "            # 2. Handle duplicates (e.g., if 'Price a' and 'Price b' both become 'Price')\n",
    "            if clean_name in seen:\n",
    "                seen[clean_name] += 1\n",
    "                clean_name = f\"{clean_name}_{seen[clean_name]}\"\n",
    "            else:\n",
    "                seen[clean_name] = 0\n",
    "            new_cols.append(clean_name)\n",
    "    df.columns = new_cols\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aaaf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect specific stuff like snp and pvalue\n",
    "def contains_valid_snp(row):\n",
    "    row_snp_pattern = r\"\\b(?:rs|s)\\d+\\S*\"\n",
    "    return any(re.search(row_snp_pattern, str(value)) for value in row)\n",
    "\n",
    "def contains_valid_pvalue(row):\n",
    "    row_pvalue_pattern = r\"\\d+\\.\\d+\"\n",
    "    return any(re.search(row_pvalue_pattern, str(value)) for value in row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a478c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract table list\n",
    "def extract_tables_lst_from_pdf_and_num_col(file_name, tables_num_col_lst):\n",
    "    reader = PdfReader(file_name)\n",
    "    options = PdfPipelineOptions()\n",
    "    options.table_structure_options.mode = TableFormerMode.ACCURATE\n",
    "    converter = DocumentConverter(\n",
    "        format_options={\n",
    "            InputFormat.PDF: PdfFormatOption(pipeline_options=options)\n",
    "        }\n",
    "    )\n",
    "    df_lst = []\n",
    "    row_snp_pattern = r\"\\b(?:rs|s)\\d+\\S*\"\n",
    "    row_pvalue_pattern = r\"\\d+\\.\\d+\"\n",
    "\n",
    "    page_num = 1\n",
    "    while len(df_lst) < len(tables_num_col_lst) and page_num <= len(reader.pages):\n",
    "        for angle in [0, 90]: # Try normal, then try rotated\n",
    "            writer = PdfWriter()\n",
    "            page = reader.pages[page_num - 1]\n",
    "            \n",
    "            if angle != 0:\n",
    "                page.rotate(angle)\n",
    "            \n",
    "            writer.add_page(page)\n",
    "            \n",
    "            # Convert just this one page\n",
    "            pdf_buffer = io.BytesIO()\n",
    "            writer.write(pdf_buffer)\n",
    "            pdf_buffer.seek(0)\n",
    "            \n",
    "            doc_stream = DocumentStream(name=f\"page_{page_num}.pdf\", stream=pdf_buffer)\n",
    "            result = converter.convert(doc_stream)\n",
    "            \n",
    "            # Check if this rotation produced valid table rows\n",
    "            temp_dfs = []\n",
    "            for table in result.document.tables:\n",
    "                df = table.export_to_dataframe()\n",
    "                # check if table is empty\n",
    "                if (not df.empty):\n",
    "                    # Case 1: continue from previous table\n",
    "                    if len(df_lst) > 0 and df.shape[1] == df_lst[-1].shape[1]:\n",
    "                        # extra filters for tables that are snp related, we need to remove rows that do not have snp id\n",
    "                        # often are separation between sections\n",
    "                        for col in df.columns:\n",
    "                            # first modify \"\" -> nan\n",
    "                            df[col] = df[col].replace(r'^\\s*$', np.nan, regex=True).ffill()\n",
    "                        df[\"valid_row\"] = df.apply(lambda x: contains_valid_snp(x) and contains_valid_pvalue(x), axis=1)\n",
    "                        df = df[df[\"valid_row\"]].drop(\"valid_row\", axis=1).reset_index().drop(\"index\", axis = 1)\n",
    "                        # some cell have the tag the end (often include a space and a small letter)\n",
    "                        df = df.map(clean_cell) \n",
    "                        df = clean_headers(df) \n",
    "                        if df_lst[-1].columns.equals(df.columns):\n",
    "                            df_lst[-1] = pd.concat([df_lst[-1], df], ignore_index = True)\n",
    "                        elif df.shape[1] == tables_num_col_lst[len(df_lst) + len(temp_dfs)]:\n",
    "                            # fail that test => add to temp since this is a new table\n",
    "                            temp_dfs.append(df)\n",
    "                    # Case 2: new table\n",
    "                    elif (len(df_lst) + len(temp_dfs)) < len(tables_num_col_lst) and df.shape[1] == tables_num_col_lst[len(df_lst) + len(temp_dfs)]: \n",
    "                        # extra filters for tables that are snp related, we need to remove rows that do not have snp id\n",
    "                        # often are separation between sections\n",
    "                        for col in df.columns:\n",
    "                            # first modify \"\" -> nan\n",
    "                            df[col] = df[col].replace(r'^\\s*$', np.nan, regex=True).ffill()\n",
    "                        df[\"valid_row\"] = df.apply(lambda x: contains_valid_snp(x) and contains_valid_pvalue(x), axis=1)\n",
    "                        df = df[df[\"valid_row\"]].drop(\"valid_row\", axis=1).reset_index().drop(\"index\", axis = 1)\n",
    "                        # some cell have the tag the end (often include a space and a small letter)\n",
    "                        df = df.map(clean_cell) \n",
    "                        df = clean_headers(df)             \n",
    "                        temp_dfs.append(df)\n",
    "            if temp_dfs:\n",
    "                df_lst.extend(temp_dfs)\n",
    "                break\n",
    "            \n",
    "        page_num += 1\n",
    "\n",
    "    return df_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bd8b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tables_lst_from_paper(pmcid, file_name, table_inx_to_extract=[]):\n",
    "    tables_num_col_lst = extract_tables_num_col_lst_from_pmc(pmcid)\n",
    "    if len(table_inx_to_extract) > 0:\n",
    "        tables_num_col_lst = [tables_num_col_lst[i] for i in table_inx_to_extract]\n",
    "    df_lst = extract_tables_lst_from_pdf_and_num_col(file_name, tables_num_col_lst)\n",
    "    return df_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6513d3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmcid = \"PMC10497850\"\n",
    "file_name = \"papers/ACEL-22-e13938.pdf\"\n",
    "df_lst = extract_tables_lst_from_paper(pmcid, file_name)\n",
    "for i in [3, 4]:\n",
    "    df_lst[i-1].to_csv(f\"tables/{file_name.split('/')[-1].replace('.pdf', '')}_table_{i}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b4dc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmcid = \"PMC10115645\"\n",
    "file_name = \"papers/41591_2023_Article_2268.pdf\"  # Can be a local path or a URL\n",
    "df_lst = extract_tables_lst_from_paper(pmcid, file_name)\n",
    "for i in [1, 2]:\n",
    "    df_lst[i-1].to_csv(f\"tables/{file_name.split('/')[-1].replace('.pdf', '')}_table_{i}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c76427",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmcid = \"PMC9622429\"\n",
    "file_name = \"papers/nihms-1797266.pdf\"  # Can be a local path or a URL\n",
    "df_lst = extract_tables_lst_from_paper(pmcid, file_name)  \n",
    "for i in [3, 5, 6, 7, 8]:\n",
    "    df_lst[i-1].to_csv(f\"tables/{file_name.split('/')[-1].replace('.pdf', '')}_table_{i}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf1ea7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmcid = \"PMC10615750\"\n",
    "file_name = \"papers/Recent paper on AD GWAS (1).pdf\"  # Can be a local path or a URL\n",
    "df_lst = extract_tables_lst_from_paper(pmcid, file_name)  \n",
    "for i in [1, 2]:\n",
    "    df_lst[i-1].to_csv(f\"tables/{file_name.split('/')[-1].replace('.pdf', '')}_table_{i}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42ae61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmcid = \"PMC6677735\"\n",
    "file_name = \"papers/s42003-019-0537-9.pdf\"  # Can be a local path or a URL\n",
    "df_lst = extract_tables_lst_from_paper(pmcid, file_name)  \n",
    "for i in [1]:\n",
    "    df_lst[i-1].to_csv(f\"tables/{file_name.split('/')[-1].replace('.pdf', '')}_table_{i}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b843b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmcid = \"PMC10286470\"\n",
    "file_name = \"papers/s13024-023-00633-4.pdf\"  # Can be a local path or a URL\n",
    "df_lst = extract_tables_lst_from_paper(pmcid, file_name)  \n",
    "for i in [2, 3]:\n",
    "    df_lst[i-1].to_csv(f\"tables/{file_name.split('/')[-1].replace('.pdf', '')}_table_{i}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bce5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pmcid = \"PMC9945061\"\n",
    "# file_name = \"PMID36809323_table_3.pdf\"\n",
    "# df_lst = extract_tables_lst_from_paper(pmcid, file_name)  \n",
    "# for i in [3]:\n",
    "#     df_lst[i-3].to_csv(f\"tables/{file_name.split('/')[-1].replace('.pdf', '')}_table_{i}.csv\", index=False)\n",
    "# problem in PMC api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c135bb",
   "metadata": {},
   "source": [
    "Try to map columns to reference columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0755935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try to map the columns with the actual col in advp\n",
    "# referencing_cols = pd.read_csv(\"Rules for harmonizing ADVP papers - Main cols.csv\")\n",
    "# referencing_cols[\"column_with_context\"] = referencing_cols.apply(lambda x: x[\"column\"] if pd.isna(x[\"description\"]) else x[\"column\"] + \": \" + x[\"description\"], axis = 1)\n",
    "# # referencing_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf842b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NOTE: before working in the main cell for mapping columns, write inference code here + setting up models here\n",
    "# def create_embeddings_from_model(sentences, model, tokenizer):\n",
    "#     input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "#     # get token embeddings\n",
    "#     with torch.no_grad():\n",
    "#         output = model(**input)\n",
    "#     token_embeddings = output[0]\n",
    "\n",
    "#     # extract mask and mean pooling for sentence embeddings\n",
    "#     input_mask_expanded = input['attention_mask'].unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "#     sentence_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "#     # final normalization\n",
    "#     sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "#     return sentence_embeddings\n",
    "\n",
    "# # setting up models\n",
    "# embeddings_model_name = \"NeuML/pubmedbert-base-embeddings\"\n",
    "# embeddings_model_tokenizer = AutoTokenizer.from_pretrained(embeddings_model_name)\n",
    "# embeddings_model = AutoModel.from_pretrained(embeddings_model_name)\n",
    "# embeddings_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4de235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SingleTokenBiasProcessor(LogitsProcessor):\n",
    "#     def __init__(self, token_ids, bias_value):\n",
    "#         self.token_ids = token_ids\n",
    "#         self.bias_value = bias_value\n",
    "\n",
    "#     def __call__(self, input_ids, scores):\n",
    "#         # Create a mask for allowed tokens\n",
    "#         mask = torch.full_like(scores, -float(\"inf\"))\n",
    "#         for tid in self.token_ids:\n",
    "#             mask[:, tid] = self.bias_value\n",
    "#         return scores + mask\n",
    "\n",
    "# def reranking_with_llm(col, candidates, llm_model, llm_model_tokenizer):\n",
    "#     \"\"\"\n",
    "#     LLM acts as a re-ranker to pick the best match from a list of candidates.\n",
    "#     \"\"\"\n",
    "#     # Format the candidates as a numbered list for the LLM\n",
    "\n",
    "#     prompt = f\"\"\"Task: Map clinical table headers to GWAS standard ontology, return a single number for the best choice\n",
    "\n",
    "# Header: \"p-value: 0.001, 5e-8, 0.43\"\n",
    "# Candidates: \n",
    "#     1. P-value: The statistical significance of the association. Keywords: P, P-value, P_adj, FDR. Examples: 5.0E-08, 0.0012, 1.2 x 10^-5, 0.05.\n",
    "#     2. Effect Size: The magnitude and direction of the association. Keywords: Beta, OR, HR, Estimate. Examples: Beta=0.25, OR=1.45, HR=1.12, Log(OR)=0.37.\n",
    "#     3. SNP: Variant identifier, or snp idenifier, or chr:pos. Keywords: chr:position, chr:pos, Variant, rsID, RS number, MarkerName, rs. Examples: rs12345, 20:45269867, 19:45411941:T:C, chr19:45411941, rs429358 (APOE ε4).\n",
    "# Best Match: 1\n",
    "\n",
    "# Header: \"rs_number: rs123, rs456, rs789\"\n",
    "# Candidates: \n",
    "#     1. Chr: Genomic chromosome identifier. Keywords: CHR, Chrom, Chromosome. Examples: 1, 19, X, chr19, chrX.\n",
    "#     2. Position: Genomic coordinate location. Keywords: BP, POS, Base Pair, start, end. Examples: 45411941, 10240500:10248600 (range), build 37.\n",
    "#     3. SNP: Variant identifier, or snp idenifier, or chr:pos. Keywords: chr:position, chr:pos, Variant, rsID, RS number, MarkerName, rs. Examples: rs12345, 20:45269867, 19:45411941:T:C, chr19:45411941, rs429358 (APOE ε4).\n",
    "# Best Match: 2\n",
    "\n",
    "# Header: \"{col}\"\n",
    "# Candidates: \n",
    "#     - {candidates[0]}\n",
    "#     - {candidates[1]}\n",
    "#     - {candidates[2]}\n",
    "# Best Match: \"\"\"\n",
    "\n",
    "#     allowed_indices = [str(i+1) for i in range(len(candidates))]\n",
    "#     allowed_token_ids = [llm_model_tokenizer.encode(idx, add_special_tokens=False)[0] for idx in allowed_indices]\n",
    "    \n",
    "#     # logit bias to limit tokens that can be output\n",
    "#     bias_processor = SingleTokenBiasProcessor(allowed_token_ids, 100.0)\n",
    "#     logits_processor = LogitsProcessorList([bias_processor])\n",
    "\n",
    "#     # 4. Generate exactly ONE token\n",
    "#     inputs = llm_model_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         output = llm_model.generate(\n",
    "#             **inputs,\n",
    "#             max_new_tokens=1,      # Force exactly one token\n",
    "#             logits_processor=logits_processor, # Force it to be one of our numbers\n",
    "#             pad_token_id=llm_model_tokenizer.eos_token_id,\n",
    "#             do_sample=False        # Greedy decoding for consistency\n",
    "#         )\n",
    "\n",
    "#     # 5. Extract and Convert to Integer\n",
    "#     new_token = output[0][-1]\n",
    "#     predicted_text = llm_model_tokenizer.decode(new_token).strip()\n",
    "    \n",
    "#     try:\n",
    "#         idx = int(predicted_text) - 1 # Convert back to 0-based list index\n",
    "#         return candidates[idx]\n",
    "#     except (ValueError, IndexError):\n",
    "#         return col\n",
    "\n",
    "# llm_model_name = \"stanford-crfm/BioMedLM\"\n",
    "# llm_model_tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "# llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     llm_model_name,\n",
    "#     torch_dtype=torch.bfloat16\n",
    "# ).to(device)\n",
    "# llm_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f2831b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Try to first convert any abbreviation, then we match column with right semantic\n",
    "# gwas_abbreviation_dict = {\n",
    "#     \"CHR\": \"Chromosome number\",\n",
    "#     \"BP\": \"Base-pair position\",\n",
    "#     \"POS\": \"Position\",\n",
    "#     \"SNP\": \"Single nucleotide polymorphism identifier\",\n",
    "#     \"RS\": \"Reference Single nucleotide polymorphism\",\n",
    "#     \"VAR\": \"Variant\",\n",
    "#     \"ID\": \"identifier\",\n",
    "#     \"A1\": \"Effect allele / tested allele\",\n",
    "#     \"A2\": \"Other allele / non-effect allele\",\n",
    "#     \"REF\": \"Reference allele (genome reference)\",\n",
    "#     \"ALT\": \"Alternate allele\",\n",
    "#     \"EA\": \"Effect Allele\",\n",
    "#     \"NEA\": \"Non-Effect Allele\",\n",
    "#     \"RA\": \"Risk Allele\",\n",
    "#     \"OA\": \"Other Allele\",\n",
    "#     \"AF\": \"Allele Frequency (general term)\",\n",
    "#     \"RAF\": \"Risk Allele Frequency\",\n",
    "#     \"EAF\": \"Effect Allele Frequency\",\n",
    "#     \"MAF\": \"Minor Allele Frequency\",\n",
    "#     \"BETA\": \"Effect size (regression coefficient)\",\n",
    "#     \"OR\": \"Odds Ratio\",\n",
    "#     \"SE\": \"Standard Error of effect estimate\",\n",
    "#     \"Z\": \"Z-score statistic\",\n",
    "#     \"T\": \"T-statistic\",\n",
    "#     \"CI\": \"Confidence Interval\",\n",
    "#     \"P\": \"P-value\",\n",
    "#     \"PVAL\": \"P-value\",\n",
    "#     \"LOGP\": \"Negative log10 P-value\",\n",
    "#     \"Q\": \"Heterogeneity statistic (meta-analysis)\",\n",
    "#     \"I2\": \"I-squared heterogeneity metric\",\n",
    "#     \"HWE\": \"Hardy-Weinberg Equilibrium test statistic\",\n",
    "#     \"INFO\": \"Imputation quality score\",\n",
    "#     \"R2\": \"Imputation accuracy metric\",\n",
    "#     \"CALLRATE\": \"Genotype call rate\",\n",
    "#     \"MISSING\": \"Missing genotype rate\",\n",
    "#     \"N\": \"Total sample size\",\n",
    "#     \"N_CASES\": \"Number of cases (for binary traits)\",\n",
    "#     \"N_CONTROLS\": \"Number of controls (for binary traits)\",\n",
    "#     \"EUR\": \"European ancestry\",\n",
    "#     \"AFR\": \"African ancestry\",\n",
    "#     \"ASN\": \"Asian ancestry\",\n",
    "#     \"AMR\": \"Admixed American ancestry\",\n",
    "#     \"SAS\": \"South Asian ancestry\",\n",
    "#     \"EAS\": \"East Asian ancestry\",\n",
    "#     \"LD\": \"Linkage Disequilibrium\",\n",
    "#     \"DPRIME\": \"LD D’ value\",\n",
    "#     \"CADD\": \"CADD score (functional impact)\",\n",
    "#     \"EQTL\": \"Expression quantitative trait locus\",\n",
    "#     \"PQTL\": \"Protein QTL\",\n",
    "#     \"GWGAS\": \"Gene-wide association study\",\n",
    "#     \"PRS\": \"Polygenic Risk Score\",\n",
    "#     \"PGS\": \"Polygenic Score\",\n",
    "#     \"QC\": \"Quality Control\",\n",
    "#     \"MA\": \"Meta-analysis\",\n",
    "#     \"HLA\": \"Human Leukocyte Antigen region\",\n",
    "#     \"HR\": \"Hazard ratio\",\n",
    "#     \"HET\": \"Heterogeneity test\",\n",
    "#     \"APOE4\": \"APOE ε4\",\n",
    "#     \"APOE*4\": \"APOE ε4\",\n",
    "#     \"#\": \"Number of\",\n",
    "#     \"frq\": \"Frequency\",\n",
    "#     'β': \"Effect\",\n",
    "#     \"nsnps\": \"Number of Variants\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df00f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_col(col):\n",
    "#     \"\"\"\n",
    "#     Clean a column by replacing any possible abbreviation with their actual meaning for better semantic matching\n",
    "#     \"\"\"\n",
    "#     new_col = col\n",
    "#     for abb in gwas_abbreviation_dict:\n",
    "#         if re.search(fr\"[^a-zA-Z]{abb.lower()}[^a-zA-Z]\", new_col.lower()):\n",
    "#             new_col = re.sub(fr\"([^a-zA-Z]){abb.lower()}([^a-zA-Z])\", fr\"\\1{gwas_abbreviation_dict[abb]}\\2\", new_col.lower())\n",
    "#         elif re.search(fr\"^{abb.lower()}[^a-zA-Z]\", new_col.lower()):\n",
    "#             new_col = re.sub(fr\"^{abb.lower()}([^a-zA-Z])\", fr\"{gwas_abbreviation_dict[abb]}\\1\", new_col.lower())\n",
    "#         elif re.search(fr\"[^a-zA-Z]{abb.lower()}$\", new_col.lower()):\n",
    "#             new_col = re.sub(fr\"([^a-zA-Z]){abb.lower()}$\", fr\"\\1{gwas_abbreviation_dict[abb]}\", new_col.lower())\n",
    "#         elif re.search(fr\"^{abb.lower()}$\", new_col.lower()):\n",
    "#             new_col = re.sub(fr\"{abb.lower()}\", gwas_abbreviation_dict[abb], new_col.lower())\n",
    "#     # new_col = new_col.replace(\".\", \" \")\n",
    "#     return new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c45269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_col_prompt(col, example_values, num_example_values = 5):\n",
    "#     \"\"\"\n",
    "#     Based on the column title and some possible values of that columns, try to make a prompt\n",
    "#     \"\"\"\n",
    "#     col_prompt = f\"{col}: \"\n",
    "#     for i in range(min(num_example_values, len(example_values))):\n",
    "#         col_prompt += f\"{example_values[i]}, \"\n",
    "#     return col_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a589c8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def match_single_col_to_ref_col(col, ref_col_lst, ref_col_embeddings, embeddings_model, embeddings_model_tokenizer):\n",
    "#     \"\"\"\n",
    "#     match a single column to reference col given column, the list of referencing col and their embeddings\n",
    "#     \"\"\"\n",
    "#     # calculate column embedding\n",
    "#     # col_embeddings = embeddings_model.encode(col, normalize_embeddings=True)\n",
    "#     # multi_index_pattern = r\".+\\..+\"\n",
    "#     # if re.search(multi_index_pattern, col):\n",
    "#     #     col = col.replace(\".\", \" \")\n",
    "#     #     # sub_col_lst = col.split(\".\")\n",
    "#     #     # sub_col_embeddings = create_embeddings_from_model(sub_col_lst, embeddings_model, embeddings_model_tokenizer)\n",
    "#     #     # col_embeddings = torch.mean(sub_col_embeddings, dim = 0)\n",
    "#     #     col = col.replace(\".\", \" \")\n",
    "#     #     col_embeddings = create_embeddings_from_model(col, embeddings_model, embeddings_model_tokenizer)\n",
    "#     # else:\n",
    "#     #     col_embeddings = create_embeddings_from_model(col, embeddings_model, embeddings_model_tokenizer)\n",
    "#     col_embeddings = create_embeddings_from_model(col, embeddings_model, embeddings_model_tokenizer)\n",
    "#     col_embeddings = col_embeddings.reshape(-1, 1)\n",
    "\n",
    "#     # calculate similarity score\n",
    "#     scores = torch.matmul(ref_col_embeddings, col_embeddings).reshape(-1)\n",
    "\n",
    "#     # sort similairty score\n",
    "#     top_k_indices = torch.argsort(scores, descending=True)\n",
    "\n",
    "#     # verify if we even got good enough similarity\n",
    "#     best_inx = top_k_indices[0].item()\n",
    "#     best_score = scores[best_inx].item()\n",
    "#     # second_best_inx = top_k_indices[1].item()\n",
    "#     # second_best_score = scores[second_best_inx].item()\n",
    "#     # need a threshold for score or else, just return col\n",
    "#     # if best_score < 0.4: \n",
    "#     #     return (col, 1)\n",
    "    \n",
    "#     # # now do rerank\n",
    "#     # candidates = [ref_col_lst[inx] for inx in top_k_indices if scores[inx] >= 0.4]\n",
    "#     # candidates_scores = [scores[inx] for inx in top_k_indices if scores[inx] >= 0.4]\n",
    "#     # best_ref_col, best_ref_col_scores = reranking_from_model(col, candidates, candidates_scores)\n",
    "#     # return (best_ref_col, best_ref_col_scores)\n",
    "\n",
    "#     if best_score >= 0.4:\n",
    "#         return (ref_col_lst[best_inx], best_score)\n",
    "#     return (col, 1)\n",
    "\n",
    "# def match_many_col_to_ref_col(df, ref_col_df, embeddings_model, embeddings_model_tokenizer):\n",
    "#     \"\"\"\n",
    "#     Given a list of column and a dataframe (could be dict later on if that fits better),\n",
    "#     try to match each column to the best fitted reference col, \n",
    "#     return a dict of ref col : list of (col, cleaned col prompt, score)\n",
    "#     \"\"\"\n",
    "#     # prepare the embeddings for reference col since we reuse them\n",
    "#     ref_col_lst = ref_col_df[\"column\"].to_list()\n",
    "#     ref_col_context_lst = ref_col_df[\"column_with_context\"].to_list()\n",
    "#     ref_col_embeddings = create_embeddings_from_model(ref_col_context_lst, embeddings_model, embeddings_model_tokenizer)\n",
    "\n",
    "#     # conduct matching\n",
    "#     multi_index_pattern = r\"^.+\\..+$\" # need multi index pattern for handling multi index\n",
    "#     ref_col_to_col_lst = {}\n",
    "#     for col in df.columns:\n",
    "\n",
    "#         # extract values needed for prompt\n",
    "#         cleaned_col = clean_col(col)\n",
    "#         example_values = df[col].unique().tolist()\n",
    "#         # prompt: {col}: example, need to delete all : first\n",
    "\n",
    "#         if re.search(multi_index_pattern, cleaned_col.strip()):\n",
    "#             # try to assess each part and see if which one have highest score\n",
    "#             best_ref_col, best_score = None, 0\n",
    "#             best_cleaned_sub_col_prompt = None\n",
    "#             for sub_col in cleaned_col.split(\".\"):\n",
    "\n",
    "#                 # make column prompt for each sub col\n",
    "#                 cleaned_sub_col_prompt = make_col_prompt(sub_col, example_values)\n",
    "                \n",
    "#                 # matching and compare\n",
    "#                 ref_col, score = match_single_col_to_ref_col(cleaned_sub_col_prompt, ref_col_lst, ref_col_embeddings, embeddings_model, embeddings_model_tokenizer)\n",
    "#                 if score > best_score and ref_col != cleaned_sub_col_prompt:\n",
    "#                     best_ref_col = ref_col\n",
    "#                     best_score = score\n",
    "#                     best_cleaned_sub_col_prompt = cleaned_sub_col_prompt\n",
    "            \n",
    "#             # if we have a best one vs not\n",
    "#             if best_ref_col is not None:\n",
    "#                 if best_ref_col not in ref_col_to_col_lst:\n",
    "#                     ref_col_to_col_lst[best_ref_col] = []\n",
    "#                 ref_col_to_col_lst[best_ref_col].append((col, best_cleaned_sub_col_prompt, score))\n",
    "#             else:\n",
    "#                 if col not in ref_col_to_col_lst:\n",
    "#                     ref_col_to_col_lst[col] = []\n",
    "#                 ref_col_to_col_lst[col].append((col, best_cleaned_sub_col_prompt, 1))\n",
    "#         else:\n",
    "#             # match a single col with best fit referencing col and return a dict\n",
    "#             # make the col prompt\n",
    "#             # extra steps to remove .\n",
    "#             cleaned_col = cleaned_col.replace(\".\", \"\")\n",
    "#             cleaned_col_prompt = make_col_prompt(cleaned_col, example_values)\n",
    "\n",
    "#             # matching for best col\n",
    "#             ref_col, score = match_single_col_to_ref_col(cleaned_col_prompt, ref_col_lst, ref_col_embeddings, embeddings_model, embeddings_model_tokenizer)\n",
    "#             # if we still get same col\n",
    "#             if ref_col == cleaned_col_prompt: \n",
    "#                 ref_col = col\n",
    "#             if ref_col not in ref_col_to_col_lst:\n",
    "#                 ref_col_to_col_lst[ref_col] = []\n",
    "#             ref_col_to_col_lst[ref_col].append((col, cleaned_col_prompt, score))\n",
    "#         # ref_col, score = match_single_col_to_ref_col(cleaned_col_prompt, ref_col_lst, ref_col_embeddings)\n",
    "#         # if ref_col == cleaned_col_prompt: \n",
    "#         #     ref_col = col\n",
    "#         # if ref_col not in ref_col_to_col_lst:\n",
    "#         #     ref_col_to_col_lst[ref_col] = []\n",
    "#         # ref_col_to_col_lst[ref_col].append((col, cleaned_col_prompt, score))\n",
    "\n",
    "#     return ref_col_to_col_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c359618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def match_single_col_to_ref_col_with_llm(col, ref_col_lst, ref_col_embeddings):\n",
    "#     \"\"\"\n",
    "#     match a single column to reference col given column, the list of referencing col and their embeddings\n",
    "#     \"\"\"\n",
    "#     calculate column embedding\n",
    "#     col_embeddings = embeddings_model.encode(col, normalize_embeddings=True)\n",
    "#     multi_index_pattern = r\".+\\..+\"\n",
    "#     if re.search(multi_index_pattern, col):\n",
    "#         col = col.replace(\".\", \" \")\n",
    "#         # sub_col_lst = col.split(\".\")\n",
    "#         # sub_col_embeddings = create_embeddings_from_model(sub_col_lst, embeddings_model, embeddings_model_tokenizer)\n",
    "#         # col_embeddings = torch.mean(sub_col_embeddings, dim = 0)\n",
    "#         col = col.replace(\".\", \" \")\n",
    "#         col_embeddings = create_embeddings_from_model(col, embeddings_model, embeddings_model_tokenizer)\n",
    "#     else:\n",
    "#         col_embeddings = create_embeddings_from_model(col, embeddings_model, embeddings_model_tokenizer)\n",
    "#     col_embeddings = create_embeddings_from_model(col, embeddings_model, embeddings_model_tokenizer)\n",
    "#     col_embeddings = col_embeddings.reshape(-1, 1)\n",
    "\n",
    "#     calculate similarity score\n",
    "#     scores = torch.matmul(ref_col_embeddings, col_embeddings).reshape(-1)\n",
    "\n",
    "#     sort similairty score\n",
    "#     top_k_indices = torch.argsort(scores, descending=True)\n",
    "\n",
    "#     verify if we even got good enough similarity\n",
    "#     best_inx = top_k_indices[0].item()\n",
    "#     best_score = scores[best_inx].item()\n",
    "#     second_best_inx = top_k_indices[1].item()\n",
    "#     second_best_score = scores[second_best_inx].item()\n",
    "#     need a threshold for score or else, just return col\n",
    "#     if best_score < 0.4: \n",
    "#         return (col, 1)\n",
    "    \n",
    "#     # now do rerank\n",
    "#     candidates = [ref_col_lst[inx] for inx in top_k_indices if scores[inx] >= 0.4]\n",
    "#     candidates_scores = [scores[inx] for inx in top_k_indices if scores[inx] >= 0.4]\n",
    "#     best_ref_col, best_ref_col_scores = reranking_from_model(col, candidates, candidates_scores)\n",
    "#     return (best_ref_col, best_ref_col_scores)\n",
    "\n",
    "#     if best_score < 0.4:\n",
    "#         return col\n",
    "#     else:\n",
    "#         extract top 3 candidates\n",
    "#         candidates = []\n",
    "#         for i in range(3):\n",
    "#             inx = top_k_indices[i]\n",
    "#             candidates.append(ref_col_lst[inx])\n",
    "#         best_col = reranking_with_llm(col, candidates, llm_model, llm_model_tokenizer)\n",
    "#         return best_col\n",
    "\n",
    "# def match_many_col_to_ref_col_with_llm(df, ref_col_df):\n",
    "#     \"\"\"\n",
    "#     Given a list of column and a dataframe (could be dict later on if that fits better),\n",
    "#     try to match each column to the best fitted reference col, \n",
    "#     return a dict of ref col : list of (col, cleaned col prompt, score)\n",
    "#     \"\"\"\n",
    "#     prepare the embeddings for reference col since we reuse them\n",
    "#     ref_col_lst = ref_col_df[\"column\"].to_list()\n",
    "#     ref_col_context_lst = ref_col_df[\"column_with_context\"].to_list()\n",
    "#     ref_col_embeddings = create_embeddings_from_model(ref_col_context_lst, embeddings_model, embeddings_model_tokenizer)\n",
    "\n",
    "#     conduct matching\n",
    "#     multi_index_pattern = r\"^.+\\..+$\" # need multi index pattern for handling multi index\n",
    "#     ref_col_to_col_lst = {}\n",
    "#     for col in df.columns:\n",
    "\n",
    "#         extract values needed for prompt\n",
    "#         cleaned_col = clean_col(col)\n",
    "#         example_values = df[col].unique().tolist()\n",
    "#         prompt: {col}: example, need to delete all : first\n",
    "\n",
    "#         if re.search(multi_index_pattern, cleaned_col.strip()):\n",
    "#             try to assess each part and see if which one have highest score\n",
    "#             best_ref_col, best_score = None, 0\n",
    "#             best_cleaned_sub_col_prompt = None\n",
    "#             for sub_col in cleaned_col.split(\".\"):\n",
    "\n",
    "#                 make column prompt for each sub col\n",
    "#                 cleaned_sub_col_prompt = make_col_prompt(sub_col, example_values)\n",
    "                \n",
    "#                 matching and compare\n",
    "#                 ref_col, score = match_single_col_to_ref_col(cleaned_sub_col_prompt, ref_col_lst, ref_col_embeddings)\n",
    "#                 if score > best_score and ref_col != cleaned_sub_col_prompt:\n",
    "#                     best_ref_col = ref_col\n",
    "#                     best_score = score\n",
    "#                     best_cleaned_sub_col_prompt = cleaned_sub_col_prompt\n",
    "            \n",
    "#             if we have a best one vs not\n",
    "#             if best_ref_col is not None:\n",
    "#                 if best_ref_col not in ref_col_to_col_lst:\n",
    "#                     ref_col_to_col_lst[best_ref_col] = []\n",
    "#                 ref_col_to_col_lst[best_ref_col].append((col, best_cleaned_sub_col_prompt, score))\n",
    "#             else:\n",
    "#                 if col not in ref_col_to_col_lst:\n",
    "#                     ref_col_to_col_lst[col] = []\n",
    "#                 ref_col_to_col_lst[col].append((col, best_cleaned_sub_col_prompt, 1))\n",
    "#         else:\n",
    "#             match a single col with best fit referencing col and return a dict\n",
    "#             make the col prompt\n",
    "#             extra steps to remove .\n",
    "#             cleaned_col = cleaned_col.replace(\".\", \"\")\n",
    "#             cleaned_col_prompt = make_col_prompt(cleaned_col, example_values)\n",
    "\n",
    "#             matching for best col\n",
    "#             ref_col, score = match_single_col_to_ref_col(cleaned_col_prompt, ref_col_lst, ref_col_embeddings)\n",
    "#             if we still get same col\n",
    "#             if ref_col == cleaned_col_prompt: \n",
    "#                 ref_col = col\n",
    "#             if ref_col not in ref_col_to_col_lst:\n",
    "#                 ref_col_to_col_lst[ref_col] = []\n",
    "#             ref_col_to_col_lst[ref_col].append((col, cleaned_col_prompt, score))\n",
    "#         ref_col, score = match_single_col_to_ref_col(cleaned_col_prompt, ref_col_lst, ref_col_embeddings)\n",
    "#         if ref_col == cleaned_col_prompt: \n",
    "#             ref_col = col\n",
    "#         if ref_col not in ref_col_to_col_lst:\n",
    "#             ref_col_to_col_lst[ref_col] = []\n",
    "#         ref_col_to_col_lst[ref_col].append((col, cleaned_col_prompt, score))\n",
    "\n",
    "#     return ref_col_to_col_lst\n",
    "\n",
    "# def match_many_col_to_ref_col(df, ref_col_df, embeddings_model, embeddings_model_tokenizer):\n",
    "#     \"\"\"\n",
    "#     Given a list of column and a dataframe (could be dict later on if that fits better),\n",
    "#     try to match each column to the best fitted reference col, \n",
    "#     return a dict of ref col : list of (col, cleaned col prompt, score)\n",
    "#     \"\"\"\n",
    "#     prepare the embeddings for reference col since we reuse them\n",
    "#     ref_col_lst = ref_col_df[\"column\"].to_list()\n",
    "#     ref_col_context_lst = ref_col_df[\"column_with_context\"].to_list()\n",
    "#     ref_col_embeddings = create_embeddings_from_model(ref_col_context_lst, embeddings_model, embeddings_model_tokenizer)\n",
    "\n",
    "#     conduct matching\n",
    "#     multi_index_pattern = r\"^.+\\..+$\" # need multi index pattern for handling multi index\n",
    "#     ref_col_to_col_lst = {}\n",
    "#     for col in df.columns:\n",
    "\n",
    "#         extract values needed for prompt\n",
    "#         cleaned_col = clean_col(col)\n",
    "#         example_values = df[col].unique().tolist()\n",
    "#         prompt: {col}: example, need to delete all : first\n",
    "\n",
    "#         if re.search(multi_index_pattern, cleaned_col.strip()):\n",
    "#             try to assess each part and see if which one have highest score\n",
    "#             best_ref_col, best_score = None, 0\n",
    "#             best_cleaned_sub_col_prompt = None\n",
    "#             for sub_col in cleaned_col.split(\".\"):\n",
    "\n",
    "#                 make column prompt for each sub col\n",
    "#                 cleaned_sub_col_prompt = make_col_prompt(sub_col, example_values)\n",
    "                \n",
    "#                 matching and compare\n",
    "#                 ref_col, score = match_single_col_to_ref_col(cleaned_sub_col_prompt, ref_col_lst, ref_col_embeddings, embeddings_model, embeddings_model_tokenizer)\n",
    "#                 if score > best_score and ref_col != cleaned_sub_col_prompt:\n",
    "#                     best_ref_col = ref_col\n",
    "#                     best_score = score\n",
    "#                     best_cleaned_sub_col_prompt = cleaned_sub_col_prompt\n",
    "            \n",
    "#             if we have a best one vs not\n",
    "#             if best_ref_col is not None:\n",
    "#                 if best_ref_col not in ref_col_to_col_lst:\n",
    "#                     ref_col_to_col_lst[best_ref_col] = []\n",
    "#                 ref_col_to_col_lst[best_ref_col].append((col, best_cleaned_sub_col_prompt, score))\n",
    "#             else:\n",
    "#                 if col not in ref_col_to_col_lst:\n",
    "#                     ref_col_to_col_lst[col] = []\n",
    "#                 ref_col_to_col_lst[col].append((col, best_cleaned_sub_col_prompt, 1))\n",
    "#         else:\n",
    "#             match a single col with best fit referencing col and return a dict\n",
    "#             make the col prompt\n",
    "#             extra steps to remove .\n",
    "#             cleaned_col = cleaned_col.replace(\".\", \"\")\n",
    "#             cleaned_col_prompt = make_col_prompt(cleaned_col, example_values)\n",
    "\n",
    "#             matching for best col\n",
    "#             ref_col, score = match_single_col_to_ref_col(cleaned_col_prompt, ref_col_lst, ref_col_embeddings, embeddings_model, embeddings_model_tokenizer)\n",
    "#             if we still get same col\n",
    "#             if ref_col == cleaned_col_prompt: \n",
    "#                 ref_col = col\n",
    "#             if ref_col not in ref_col_to_col_lst:\n",
    "#                 ref_col_to_col_lst[ref_col] = []\n",
    "#             ref_col_to_col_lst[ref_col].append((col, cleaned_col_prompt, score))\n",
    "#         ref_col, score = match_single_col_to_ref_col(cleaned_col_prompt, ref_col_lst, ref_col_embeddings)\n",
    "#         if ref_col == cleaned_col_prompt: \n",
    "#             ref_col = col\n",
    "#         if ref_col not in ref_col_to_col_lst:\n",
    "#             ref_col_to_col_lst[ref_col] = []\n",
    "#         ref_col_to_col_lst[ref_col].append((col, cleaned_col_prompt, score))\n",
    "\n",
    "#     return ref_col_to_col_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251adf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in os.listdir(\"./tables\"):\n",
    "#     if \"table\" in file and \".csv\" in file and \"harmonized\" not in file:\n",
    "#         print(file)\n",
    "#         df = pd.read_csv(f\"./tables/{file}\")\n",
    "#         df.columns = ['' if 'Unnamed:' in col else col for col in df.columns]\n",
    "#         col_to_ref_col = match_many_col_to_ref_col(df, referencing_cols)\n",
    "#         for ref_col in col_to_ref_col:\n",
    "#             print(f\"{ref_col}: {col_to_ref_col[ref_col]}\")\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4e05fd",
   "metadata": {},
   "source": [
    "Rewrite the matching engine as an object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df52f137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try to map the columns with the actual col in advp\n",
    "referencing_cols = pd.read_csv(\"Rules for harmonizing ADVP papers - Main cols.csv\")\n",
    "referencing_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3c26af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleTokenBiasProcessor(LogitsProcessor):\n",
    "    def __init__(self, token_ids, bias_value):\n",
    "        self.token_ids = token_ids\n",
    "        self.bias_value = bias_value\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        # Create a mask for allowed tokens\n",
    "        mask = torch.full_like(scores, -float(\"inf\"))\n",
    "        for tid in self.token_ids:\n",
    "            mask[:, tid] = self.bias_value\n",
    "        return scores + mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8609ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GWASColumnMatchingEngine:\n",
    "    def __init__(self, referencing_col_df: pd.DataFrame, embeddings_model_name: str = \"NeuML/pubmedbert-base-embeddings\", \n",
    "                 use_llm: bool = False, llm_model_name: str = \"stanford-crfm/BioMedLM\", device: str = \"cpu\"):\n",
    "        # df of referencing col\n",
    "        if not (\"column\" in referencing_col_df.columns and \"description\" in referencing_col_df.columns):\n",
    "            raise Exception(\"Error: Dataframe for referencing columns need to have 2 columns: column and description\")\n",
    "        self.referencing_col_lst = referencing_col_df[\"column\"].to_list()\n",
    "        self.referencing_col_context_lst = referencing_col_df.apply(lambda x: x[\"column\"] if pd.isna(x[\"description\"]) else x[\"column\"] + \": \" + x[\"description\"], axis = 1).to_list()\n",
    "        if use_llm:\n",
    "            self.referencing_col_to_col_context = {c: cc for c, cc in zip(self.referencing_col_lst, self.referencing_col_context_lst)}\n",
    "            self.referencing_col_context_to_col = {cc: c for c, cc in zip(self.referencing_col_lst, self.referencing_col_context_lst)}\n",
    "\n",
    "        # Try to first convert any abbreviation, then we match column with right semantic\n",
    "        self.gwas_abbreviation_dict = {\n",
    "            \"CHR\": \"Chromosome number\",\n",
    "            \"BP\": \"Base-pair position\",\n",
    "            \"POS\": \"Position\",\n",
    "            \"SNP\": \"Single nucleotide polymorphism identifier\",\n",
    "            \"RS\": \"Reference Single nucleotide polymorphism\",\n",
    "            \"VAR\": \"Variant\",\n",
    "            \"ID\": \"identifier\",\n",
    "            \"A1\": \"Effect allele / tested allele\",\n",
    "            \"A2\": \"Other allele / non-effect allele\",\n",
    "            \"REF\": \"Reference allele (genome reference)\",\n",
    "            \"ALT\": \"Alternate allele\",\n",
    "            \"EA\": \"Effect Allele\",\n",
    "            \"NEA\": \"Non-Effect Allele\",\n",
    "            \"RA\": \"Risk Allele\",\n",
    "            \"OA\": \"Other Allele\",\n",
    "            \"AF\": \"Allele Frequency (general term)\",\n",
    "            \"RAF\": \"Risk Allele Frequency\",\n",
    "            \"EAF\": \"Effect Allele Frequency\",\n",
    "            \"MAF\": \"Minor Allele Frequency\",\n",
    "            \"BETA\": \"Effect size (regression coefficient)\",\n",
    "            \"OR\": \"Odds Ratio\",\n",
    "            \"SE\": \"Standard Error of effect estimate\",\n",
    "            \"Z\": \"Z-score statistic\",\n",
    "            \"T\": \"T-statistic\",\n",
    "            \"CI\": \"Confidence Interval\",\n",
    "            \"P\": \"P-value\",\n",
    "            \"PVAL\": \"P-value\",\n",
    "            \"LOGP\": \"Negative log10 P-value\",\n",
    "            \"Q\": \"Heterogeneity statistic (meta-analysis)\",\n",
    "            \"I2\": \"I-squared heterogeneity metric\",\n",
    "            \"HWE\": \"Hardy-Weinberg Equilibrium test statistic\",\n",
    "            \"INFO\": \"Imputation quality score\",\n",
    "            \"R2\": \"Imputation accuracy metric\",\n",
    "            \"CALLRATE\": \"Genotype call rate\",\n",
    "            \"MISSING\": \"Missing genotype rate\",\n",
    "            \"N\": \"Total sample size\",\n",
    "            \"N_CASES\": \"Number of cases (for binary traits)\",\n",
    "            \"N_CONTROLS\": \"Number of controls (for binary traits)\",\n",
    "            \"EUR\": \"European ancestry\",\n",
    "            \"AFR\": \"African ancestry\",\n",
    "            \"ASN\": \"Asian ancestry\",\n",
    "            \"AMR\": \"Admixed American ancestry\",\n",
    "            \"SAS\": \"South Asian ancestry\",\n",
    "            \"EAS\": \"East Asian ancestry\",\n",
    "            \"LD\": \"Linkage Disequilibrium\",\n",
    "            \"DPRIME\": \"LD D’ value\",\n",
    "            \"CADD\": \"CADD score (functional impact)\",\n",
    "            \"EQTL\": \"Expression quantitative trait locus\",\n",
    "            \"PQTL\": \"Protein QTL\",\n",
    "            \"GWGAS\": \"Gene-wide association study\",\n",
    "            \"PRS\": \"Polygenic Risk Score\",\n",
    "            \"PGS\": \"Polygenic Score\",\n",
    "            \"QC\": \"Quality Control\",\n",
    "            \"MA\": \"Meta-analysis\",\n",
    "            \"HLA\": \"Human Leukocyte Antigen region\",\n",
    "            \"HR\": \"Hazard ratio\",\n",
    "            \"HET\": \"Heterogeneity test\",\n",
    "            \"APOE4\": \"APOE ε4\",\n",
    "            \"APOE*4\": \"APOE ε4\",\n",
    "            \"#\": \"Number of\",\n",
    "            \"frq\": \"Frequency\",\n",
    "            'β': \"Effect\",\n",
    "            \"nsnps\": \"Number of Variants\"\n",
    "        }\n",
    "\n",
    "        # embeddings model\n",
    "        self.embeddings_model_tokenizer = AutoTokenizer.from_pretrained(embeddings_model_name)\n",
    "        self.embeddings_model = AutoModel.from_pretrained(embeddings_model_name)\n",
    "        self.embeddings_model.eval()\n",
    "\n",
    "        # also make col embeddings\n",
    "        self.referencing_col_embeddings = self.create_col_embeddings_from_model(self.referencing_col_context_lst)\n",
    "\n",
    "        # llm model\n",
    "        self.use_llm = use_llm\n",
    "        if use_llm:\n",
    "            self.llm_model_tokenizer = AutoTokenizer.from_pretrained(llm_model_name)\n",
    "            self.llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "                llm_model_name,\n",
    "                dtype=torch.bfloat16\n",
    "            ).to(device)\n",
    "            self.llm_model.eval()\n",
    "\n",
    "        # device\n",
    "        self.device = device\n",
    "\n",
    "    def clean_col(self, col: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean a column by replacing any possible abbreviation with their actual meaning for better semantic matching\n",
    "        \"\"\"\n",
    "        new_col = col\n",
    "        for abb in self.gwas_abbreviation_dict:\n",
    "            if re.search(fr\"[^a-zA-Z]{abb.lower()}[^a-zA-Z]\", new_col.lower()):\n",
    "                new_col = re.sub(fr\"([^a-zA-Z]){abb.lower()}([^a-zA-Z])\", fr\"\\1{self.gwas_abbreviation_dict[abb]}\\2\", new_col.lower())\n",
    "            elif re.search(fr\"^{abb.lower()}[^a-zA-Z]\", new_col.lower()):\n",
    "                new_col = re.sub(fr\"^{abb.lower()}([^a-zA-Z])\", fr\"{self.gwas_abbreviation_dict[abb]}\\1\", new_col.lower())\n",
    "            elif re.search(fr\"[^a-zA-Z]{abb.lower()}$\", new_col.lower()):\n",
    "                new_col = re.sub(fr\"([^a-zA-Z]){abb.lower()}$\", fr\"\\1{self.gwas_abbreviation_dict[abb]}\", new_col.lower())\n",
    "            elif re.search(fr\"^{abb.lower()}$\", new_col.lower()):\n",
    "                new_col = re.sub(fr\"{abb.lower()}\", self.gwas_abbreviation_dict[abb], new_col.lower())\n",
    "        # new_col = new_col.replace(\".\", \" \")\n",
    "        return new_col\n",
    "    \n",
    "    def make_col_prompt(self, col: str, example_values: Iterable, num_example_values: int = 5) -> str:\n",
    "        \"\"\"\n",
    "        Based on the column title and some possible values of that columns, try to make a prompt\n",
    "        \"\"\"\n",
    "        col_prompt = f\"{col}: \"\n",
    "        for i in range(min(num_example_values, len(example_values))):\n",
    "            col_prompt += f\"{example_values[i]}, \"\n",
    "        return col_prompt\n",
    "\n",
    "    def create_col_embeddings_from_model(self, col: str | List[str]) -> np.ndarray | torch.Tensor:\n",
    "        \"\"\"\n",
    "        Create embeddings from string represent col name or a prompt of that col\n",
    "        \"\"\"\n",
    "        input = self.embeddings_model_tokenizer(col, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "        # get token embeddings\n",
    "        with torch.no_grad():\n",
    "            output = self.embeddings_model(**input)\n",
    "        token_embeddings = output[0]\n",
    "\n",
    "        # extract mask and mean pooling for sentence embeddings\n",
    "        input_mask_expanded = input['attention_mask'].unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        col_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "        # final normalization\n",
    "        col_embeddings = F.normalize(col_embeddings, p=2, dim=1)\n",
    "\n",
    "        return col_embeddings\n",
    "    \n",
    "    def reranking_with_llm(self, col: str, candidates: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        LLM acts as a re-ranker to pick the best match from a list of candidates.\n",
    "        \"\"\"\n",
    "        if not self.use_llm:\n",
    "            raise Exception(\"LLM use has not been enabled in the model, please re-init with use_llm=True\")\n",
    "        \n",
    "        # Format the candidates as a numbered list for the LLM\n",
    "        candidates_str = \"\\n\".join([f\"\\t{inx+1}. {c}\" for inx, c in enumerate(candidates)])\n",
    "        prompt = f\"\"\"Task: Map clinical table headers to GWAS standard ontology, return a single number for the best choice\n",
    "\n",
    "Header: \"p-value: 0.001, 5e-8, 0.43\"\n",
    "Candidates: \n",
    "    1. P-value: The statistical significance of the association. Keywords: P, P-value, P_adj, FDR. Examples: 5.0E-08, 0.0012, 1.2 x 10^-5, 0.05.\n",
    "    2. Effect Size: The magnitude and direction of the association. Keywords: Beta, OR, HR, Estimate. Examples: Beta=0.25, OR=1.45, HR=1.12, Log(OR)=0.37.\n",
    "    3. SNP: Variant identifier, or snp idenifier, or chr:pos. Keywords: chr:position, chr:pos, Variant, rsID, RS number, MarkerName, rs. Examples: rs12345, 20:45269867, 19:45411941:T:C, chr19:45411941, rs429358 (APOE ε4).\n",
    "Best Match: 1\n",
    "\n",
    "Header: \"rs_number: rs123, rs456, rs789\"\n",
    "Candidates: \n",
    "    1. Chr: Genomic chromosome identifier. Keywords: CHR, Chrom, Chromosome. Examples: 1, 19, X, chr19, chrX.\n",
    "    2. Position: Genomic coordinate location. Keywords: BP, POS, Base Pair, start, end. Examples: 45411941, 10240500:10248600 (range), build 37.\n",
    "    3. SNP: Variant identifier, or snp idenifier, or chr:pos. Keywords: chr:position, chr:pos, Variant, rsID, RS number, MarkerName, rs. Examples: rs12345, 20:45269867, 19:45411941:T:C, chr19:45411941, rs429358 (APOE ε4).\n",
    "Best Match: 2\n",
    "\n",
    "Header: \"{col}\"\n",
    "Candidates: \n",
    "{candidates_str}\n",
    "Best Match: \"\"\"\n",
    "\n",
    "        allowed_indices = [str(i+1) for i in range(len(candidates))]\n",
    "        allowed_token_ids = [self.llm_model_tokenizer.encode(idx, add_special_tokens=False)[0] for idx in allowed_indices]\n",
    "        \n",
    "        # logit bias to limit tokens that can be output\n",
    "        bias_processor = SingleTokenBiasProcessor(allowed_token_ids, 100.0)\n",
    "        logits_processor = LogitsProcessorList([bias_processor])\n",
    "\n",
    "        # 4. Generate exactly ONE token\n",
    "        inputs = self.llm_model_tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = self.llm_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=1,      # Force exactly one token\n",
    "                logits_processor=logits_processor, # Force it to be one of our numbers\n",
    "                pad_token_id=self.llm_model_tokenizer.eos_token_id,\n",
    "                do_sample=False        # Greedy decoding for consistency\n",
    "            )\n",
    "\n",
    "        # 5. Extract and Convert to Integer\n",
    "        new_token = output[0][-1]\n",
    "        predicted_text = self.llm_model_tokenizer.decode(new_token).strip()\n",
    "        \n",
    "        try:\n",
    "            idx = int(predicted_text) - 1 # Convert back to 0-based list index\n",
    "            return candidates[idx]\n",
    "        except (ValueError, IndexError):\n",
    "            return col\n",
    "        \n",
    "    def identify_main_role_in_multi_index_col(self, col: str) -> str:\n",
    "        sub_col = col.split(\".\")\n",
    "        candidates = []\n",
    "        for i in range(len(sub_col)):\n",
    "            main_role = f\"{sub_col[i]} of \"\n",
    "            context = \" and \".join([sub_col[j] for j in range(len(sub_col)) if j != i])\n",
    "            main_role_and_context = main_role + context\n",
    "            candidates.append(main_role_and_context)\n",
    "        # make candidate str\n",
    "        candidates_str = \"\\n\".join([f\"\\t{inx+1}. {c}\" for inx, c in enumerate(candidates)])\n",
    "        prompt = f\"\"\"Task: Figure out which is the real meaning of the column in gwas table, return a number for the best choice\n",
    "\n",
    "Candidates: \n",
    "    1. Multi-ethnic meta-analysis of Effect\n",
    "    2. Effect of Multi-ethnic meta-analysis\n",
    "Best Match: 2\n",
    "\n",
    "Candidates: \n",
    "    1. p -value (10,000 permutations) of APOE *4 + females\n",
    "    2. APOE *4 + females of p -value (10,000 permutations) \n",
    "Best Match: 1\n",
    "\n",
    "Candidates:\n",
    "    1. GWASmeta-analysis or chr:position\n",
    "    2. chr:position or GWASmeta-analysis\n",
    "Best Match: 2\n",
    "\n",
    "Candidates: \n",
    "{candidates_str}\n",
    "Best Match: \"\"\"\n",
    "        \n",
    "        allowed_indices = [str(i+1) for i in range(len(candidates))]\n",
    "        allowed_token_ids = [self.llm_model_tokenizer.encode(idx, add_special_tokens=False)[0] for idx in allowed_indices]\n",
    "        \n",
    "        # logit bias to limit tokens that can be output\n",
    "        bias_processor = SingleTokenBiasProcessor(allowed_token_ids, 100.0)\n",
    "        logits_processor = LogitsProcessorList([bias_processor])\n",
    "\n",
    "        # 4. Generate exactly ONE token\n",
    "        inputs = self.llm_model_tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = self.llm_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=1,      # Force exactly one token\n",
    "                logits_processor=logits_processor, # Force it to be one of our numbers\n",
    "                pad_token_id=self.llm_model_tokenizer.eos_token_id,\n",
    "                do_sample=False        # Greedy decoding for consistency\n",
    "            )\n",
    "\n",
    "        # 5. Extract and Convert to Integer\n",
    "        new_token = output[0][-1]\n",
    "        predicted_text = self.llm_model_tokenizer.decode(new_token).strip()\n",
    "        \n",
    "        try:\n",
    "            idx = int(predicted_text) - 1 # Convert back to 0-based list index\n",
    "            return sub_col[idx]\n",
    "        except (ValueError, IndexError):\n",
    "            return col\n",
    "        \n",
    "    def match_single_col_to_ref_col(self, col: str) -> Tuple[str, float]:\n",
    "        \"\"\"\n",
    "        match a single column to reference col given column, the list of referencing col and their embeddings\n",
    "        \"\"\"\n",
    "        # calculate column embedding\n",
    "        # col_embeddings = embeddings_model.encode(col, normalize_embeddings=True)\n",
    "        # multi_index_pattern = r\".+\\..+\"\n",
    "        # if re.search(multi_index_pattern, col):\n",
    "        #     col = col.replace(\".\", \" \")\n",
    "        #     # sub_col_lst = col.split(\".\")\n",
    "        #     # sub_col_embeddings = create_embeddings_from_model(sub_col_lst, embeddings_model, embeddings_model_tokenizer)\n",
    "        #     # col_embeddings = torch.mean(sub_col_embeddings, dim = 0)\n",
    "        #     col = col.replace(\".\", \" \")\n",
    "        #     col_embeddings = create_embeddings_from_model(col, embeddings_model, embeddings_model_tokenizer)\n",
    "        # else:\n",
    "        #     col_embeddings = create_embeddings_from_model(col, embeddings_model, embeddings_model_tokenizer)\n",
    "        col_embeddings = self.create_col_embeddings_from_model(col)\n",
    "        col_embeddings = col_embeddings.reshape(-1, 1)\n",
    "\n",
    "        # calculate similarity score\n",
    "        scores = torch.matmul(self.referencing_col_embeddings, col_embeddings).reshape(-1)\n",
    "\n",
    "        # sort similairty score\n",
    "        top_k_indices = torch.argsort(scores, descending=True)\n",
    "\n",
    "        # verify if we even got good enough similarity\n",
    "        best_inx = top_k_indices[0].item()\n",
    "        best_score = scores[best_inx].item()\n",
    "        # second_best_inx = top_k_indices[1].item()\n",
    "        # second_best_score = scores[second_best_inx].item()\n",
    "        # need a threshold for score or else, just return col\n",
    "        # if best_score < 0.4: \n",
    "        #     return (col, 1)\n",
    "        \n",
    "        # # now do rerank\n",
    "        # candidates = [ref_col_lst[inx] for inx in top_k_indices if scores[inx] >= 0.4]\n",
    "        # candidates_scores = [scores[inx] for inx in top_k_indices if scores[inx] >= 0.4]\n",
    "        # best_ref_col, best_ref_col_scores = reranking_from_model(col, candidates, candidates_scores)\n",
    "        # return (best_ref_col, best_ref_col_scores)\n",
    "\n",
    "        if best_score >= 0.4:\n",
    "            return (self.referencing_col_lst[best_inx], best_score)\n",
    "        return (col, 1)\n",
    "\n",
    "    def match_many_col_to_ref_col(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"\n",
    "        Given a list of column and a dataframe (could be dict later on if that fits better),\n",
    "        try to match each column to the best fitted reference col, \n",
    "        return a dict of ref col : list of (col, cleaned col prompt, score)\n",
    "        \"\"\"\n",
    "\n",
    "        # conduct matching\n",
    "        multi_index_pattern = r\"^.+\\..+$\" # need multi index pattern for handling multi index\n",
    "        ref_col_to_col_lst = {}\n",
    "        for col in df.columns:\n",
    "\n",
    "            # extract values needed for prompt\n",
    "            cleaned_col = self.clean_col(col)\n",
    "            example_values = df[col].unique().tolist()\n",
    "            # prompt: {col}: example, need to delete all : first\n",
    "\n",
    "            if re.search(multi_index_pattern, cleaned_col.strip()):\n",
    "                # try to assess each part and see if which one have highest score\n",
    "                best_ref_col, best_score = None, 0\n",
    "                best_cleaned_sub_col_prompt = None\n",
    "                for sub_col in cleaned_col.split(\".\"):\n",
    "\n",
    "                    # make column prompt for each sub col\n",
    "                    cleaned_sub_col_prompt = self.make_col_prompt(sub_col, example_values)\n",
    "                    \n",
    "                    # matching and compare\n",
    "                    ref_col, score = self.match_single_col_to_ref_col(cleaned_sub_col_prompt)\n",
    "                    if score > best_score and ref_col != cleaned_sub_col_prompt:\n",
    "                        best_ref_col = ref_col\n",
    "                        best_score = score\n",
    "                        best_cleaned_sub_col_prompt = cleaned_sub_col_prompt\n",
    "                \n",
    "                # if we have a best one vs not\n",
    "                if best_ref_col is not None:\n",
    "                    if best_ref_col not in ref_col_to_col_lst:\n",
    "                        ref_col_to_col_lst[best_ref_col] = []\n",
    "                    ref_col_to_col_lst[best_ref_col].append((col, best_cleaned_sub_col_prompt, score))\n",
    "                else:\n",
    "                    if col not in ref_col_to_col_lst:\n",
    "                        ref_col_to_col_lst[col] = []\n",
    "                    ref_col_to_col_lst[col].append((col, best_cleaned_sub_col_prompt, 1))\n",
    "            else:\n",
    "                # match a single col with best fit referencing col and return a dict\n",
    "                # make the col prompt\n",
    "                # extra steps to remove .\n",
    "                cleaned_col = cleaned_col.replace(\".\", \"\")\n",
    "                cleaned_col_prompt = self.make_col_prompt(cleaned_col, example_values)\n",
    "\n",
    "                # matching for best col\n",
    "                ref_col, score = self.match_single_col_to_ref_col(cleaned_col_prompt)\n",
    "                # if we still get same col\n",
    "                if ref_col == cleaned_col_prompt: \n",
    "                    ref_col = col\n",
    "                if ref_col not in ref_col_to_col_lst:\n",
    "                    ref_col_to_col_lst[ref_col] = []\n",
    "                ref_col_to_col_lst[ref_col].append((col, cleaned_col_prompt, score))\n",
    "            # ref_col, score = match_single_col_to_ref_col(cleaned_col_prompt, ref_col_lst, ref_col_embeddings)\n",
    "            # if ref_col == cleaned_col_prompt: \n",
    "            #     ref_col = col\n",
    "            # if ref_col not in ref_col_to_col_lst:\n",
    "            #     ref_col_to_col_lst[ref_col] = []\n",
    "            # ref_col_to_col_lst[ref_col].append((col, cleaned_col_prompt, score))\n",
    "\n",
    "        return ref_col_to_col_lst\n",
    "    \n",
    "    def match_single_col_to_ref_col_with_llm(self, col: str, num_candidates: int = 3) -> str:\n",
    "        \"\"\"\n",
    "        match a single column to reference col given column, the list of referencing col and their embeddings\n",
    "        \"\"\"\n",
    "        if not self.use_llm:\n",
    "            raise Exception(\"LLM use has not been enabled in the model, please re-init with use_llm=True\")\n",
    "        \n",
    "        # calculate column embedding\n",
    "        # col_embeddings = embeddings_model.encode(col, normalize_embeddings=True)\n",
    "        # multi_index_pattern = r\".+\\..+\"\n",
    "        # if re.search(multi_index_pattern, col):\n",
    "        #     col = col.replace(\".\", \" \")\n",
    "        #     # sub_col_lst = col.split(\".\")\n",
    "        #     # sub_col_embeddings = create_embeddings_from_model(sub_col_lst, embeddings_model, embeddings_model_tokenizer)\n",
    "        #     # col_embeddings = torch.mean(sub_col_embeddings, dim = 0)\n",
    "        #     col = col.replace(\".\", \" \")\n",
    "        #     col_embeddings = create_embeddings_from_model(col, embeddings_model, embeddings_model_tokenizer)\n",
    "        # else:\n",
    "        #     col_embeddings = create_embeddings_from_model(col, embeddings_model, embeddings_model_tokenizer)\n",
    "        col_embeddings = self.create_col_embeddings_from_model(col)\n",
    "        col_embeddings = col_embeddings.reshape(-1, 1)\n",
    "\n",
    "        # calculate similarity score\n",
    "        scores = torch.matmul(self.referencing_col_embeddings, col_embeddings).reshape(-1)\n",
    "\n",
    "        # sort similairty score\n",
    "        top_k_indices = torch.argsort(scores, descending=True)\n",
    "\n",
    "        # verify if we even got good enough similarity\n",
    "        best_inx = top_k_indices[0].item()\n",
    "        best_score = scores[best_inx].item()\n",
    "        # second_best_inx = top_k_indices[1].item()\n",
    "        # second_best_score = scores[second_best_inx].item()\n",
    "        # need a threshold for score or else, just return col\n",
    "        # if best_score < 0.4: \n",
    "        #     return (col, 1)\n",
    "        \n",
    "        # # now do rerank\n",
    "        # candidates = [ref_col_lst[inx] for inx in top_k_indices if scores[inx] >= 0.4]\n",
    "        # candidates_scores = [scores[inx] for inx in top_k_indices if scores[inx] >= 0.4]\n",
    "        # best_ref_col, best_ref_col_scores = reranking_from_model(col, candidates, candidates_scores)\n",
    "        # return (best_ref_col, best_ref_col_scores)\n",
    "\n",
    "        if best_score < 0.4:\n",
    "            return col\n",
    "        else:\n",
    "            # extract top 3 candidates\n",
    "            candidates = []\n",
    "            for i in range(num_candidates):\n",
    "                inx = top_k_indices[i]\n",
    "                # need to add the column with context, not just column\n",
    "                candidates.append(self.referencing_col_context_lst[inx])\n",
    "            best_col = self.reranking_with_llm(col, candidates)\n",
    "            # after this, map back to normal column\n",
    "            if best_col in self.referencing_col_context_to_col:\n",
    "                best_col = self.referencing_col_context_to_col[best_col]\n",
    "            return best_col\n",
    "\n",
    "    def match_many_col_to_ref_col_with_llm(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"\n",
    "        Given a list of column and a dataframe (could be dict later on if that fits better),\n",
    "        try to match each column to the best fitted reference col, \n",
    "        return a dict of ref col : list of (col, cleaned col prompt, score)\n",
    "        \"\"\"\n",
    "        if not self.use_llm:\n",
    "            raise Exception(\"LLM use has not been enabled in the model, please re-init with use_llm=True\")\n",
    "        \n",
    "        # prepare the embeddings for reference col since we reuse them\n",
    "        # conduct matching\n",
    "        multi_index_pattern = r\"^.+\\..+$\" # need multi index pattern for handling multi index\n",
    "        ref_col_to_col_lst = {}\n",
    "        for col in df.columns:\n",
    "\n",
    "            # extract values needed for prompt\n",
    "            cleaned_col = self.clean_col(col)\n",
    "            example_values = df[col].unique().tolist()\n",
    "            # prompt: {col}: example, need to delete all : first\n",
    "\n",
    "            # if re.search(multi_index_pattern, cleaned_col.strip()):\n",
    "            #     # try to assess each part and see if which one have highest score\n",
    "            #     best_candidates = []\n",
    "            #     for sub_col in cleaned_col.split(\".\"):\n",
    "\n",
    "            #         # make column prompt for each sub col\n",
    "            #         cleaned_sub_col_prompt = self.make_col_prompt(sub_col, example_values)\n",
    "                    \n",
    "            #         # matching and compare\n",
    "            #         ref_col = self.match_single_col_to_ref_col_with_llm(cleaned_sub_col_prompt)\n",
    "            #         # need to convert because we do the reranking again later, we must convert to col with context\n",
    "            #         if ref_col in self.referencing_col_to_col_context:\n",
    "            #             best_candidates.append(self.referencing_col_to_col_context[ref_col])\n",
    "            #         else:\n",
    "            #             best_candidates.append(ref_col)\n",
    "            #     # if we have a best one vs not\n",
    "            #     if len(best_candidates) > 0:\n",
    "            #         cleaned_col = cleaned_col.replace(\".\", \" \")\n",
    "            #         cleaned_col_prompt = self.make_col_prompt(cleaned_col, example_values)\n",
    "            #         best_ref_col = self.reranking_with_llm(cleaned_col_prompt, best_candidates)\n",
    "\n",
    "            #         # after getting best ref col, since it is still col with context, need converting back\n",
    "            #         if best_ref_col in self.referencing_col_context_to_col:\n",
    "            #             best_ref_col = self.referencing_col_context_to_col[best_ref_col]\n",
    "\n",
    "            #         if best_ref_col not in ref_col_to_col_lst:\n",
    "            #             ref_col_to_col_lst[best_ref_col] = []\n",
    "            #         ref_col_to_col_lst[best_ref_col].append((col, cleaned_col_prompt))\n",
    "            #     else:\n",
    "            #         if col not in ref_col_to_col_lst:\n",
    "            #             ref_col_to_col_lst[col] = []\n",
    "            #         ref_col_to_col_lst[col].append((col, None))\n",
    "            # else:\n",
    "            #     # match a single col with best fit referencing col and return a dict\n",
    "            #     # make the col prompt\n",
    "            #     # extra steps to remove .\n",
    "            #     cleaned_col = cleaned_col.replace(\".\", \"\")\n",
    "            #     cleaned_col_prompt = self.make_col_prompt(cleaned_col, example_values)\n",
    "\n",
    "            #     # matching for best col\n",
    "            #     ref_col = self.match_single_col_to_ref_col_with_llm(cleaned_col_prompt)\n",
    "            #     # if we still get same col\n",
    "            #     if ref_col == cleaned_col_prompt: \n",
    "            #         ref_col = col\n",
    "            #     if ref_col not in ref_col_to_col_lst:\n",
    "            #         ref_col_to_col_lst[ref_col] = []\n",
    "            #     ref_col_to_col_lst[ref_col].append((col, cleaned_col_prompt))\n",
    "            # ref_col, score = match_single_col_to_ref_col(cleaned_col_prompt, ref_col_lst, ref_col_embeddings)\n",
    "            # if ref_col == cleaned_col_prompt: \n",
    "            #     ref_col = col\n",
    "            # if ref_col not in ref_col_to_col_lst:\n",
    "            #     ref_col_to_col_lst[ref_col] = []\n",
    "            # ref_col_to_col_lst[ref_col].append((col, cleaned_col_prompt, score))\n",
    "\n",
    "            if re.search(multi_index_pattern, cleaned_col.strip()):\n",
    "                main_role = self.identify_main_role_in_multi_index_col(cleaned_col)\n",
    "                cleaned_col_prompt = self.make_col_prompt(main_role, example_values)\n",
    "            else:\n",
    "                cleaned_col = cleaned_col.replace(\".\", \"\")\n",
    "                cleaned_col_prompt = self.make_col_prompt(cleaned_col, example_values)\n",
    "            ref_col, score = self.match_single_col_to_ref_col(cleaned_col_prompt)\n",
    "            # if we still get same col\n",
    "            if ref_col == cleaned_col_prompt: \n",
    "                ref_col = col\n",
    "            if ref_col not in ref_col_to_col_lst:\n",
    "                ref_col_to_col_lst[ref_col] = []\n",
    "            ref_col_to_col_lst[ref_col].append((col, cleaned_col_prompt, score))\n",
    "\n",
    "        return ref_col_to_col_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408c1a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwas_col_matching_engine = GWASColumnMatchingEngine(referencing_cols, use_llm = True, device = device)\n",
    "\n",
    "for file in os.listdir(\"./tables\"):\n",
    "    if \"table\" in file and \".csv\" in file and \"harmonized\" not in file:\n",
    "        print(file)\n",
    "        df = pd.read_csv(f\"./tables/{file}\")\n",
    "        df.columns = ['' if 'Unnamed:' in col else col for col in df.columns]\n",
    "        col_to_ref_col = gwas_col_matching_engine.match_many_col_to_ref_col(df)\n",
    "        for ref_col in col_to_ref_col:\n",
    "            print(f\"{ref_col}: {col_to_ref_col[ref_col]}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8012a639",
   "metadata": {},
   "source": [
    "Final formatting of the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a8ed1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now can try to use these dictionary to make a final dataset\n",
    "# need a list of columns that we sure that there might be multiple copies of it that we can melt them into many rows\n",
    "possible_ref_col_to_melt = [\"P-value\", \"Effect Size\", \"AF\"]\n",
    "gwas_col_matching_engine = GWASColumnMatchingEngine(referencing_cols)\n",
    "def format_original_table(df, remove_unique_col = False):\n",
    "    # map the columns\n",
    "    new_col_to_old_col_lst = gwas_col_matching_engine.match_many_col_to_ref_col(df)\n",
    "    df_with_ref_col = None \n",
    "    new_col_to_not_melt = [] # list of columns that are stable and not need to be melt\n",
    "    new_col_to_old_col_lst_to_melt = {}\n",
    "    for new_col in new_col_to_old_col_lst:\n",
    "        if len(new_col_to_old_col_lst[new_col]) == 1:\n",
    "            if (not remove_unique_col) or (remove_unique_col and new_col_to_old_col_lst[new_col][0][2] != 1):\n",
    "                if df_with_ref_col is None:\n",
    "                    df_with_ref_col = df[[new_col_to_old_col_lst[new_col][0][0]]]\n",
    "                    df_with_ref_col = df_with_ref_col.rename({new_col_to_old_col_lst[new_col][0][0]: new_col}, axis = 1)\n",
    "                else:\n",
    "                    df_with_ref_col[new_col] = df[new_col_to_old_col_lst[new_col][0][0]]\n",
    "                # add these single col to the list of not melt\n",
    "                new_col_to_not_melt.append(new_col)\n",
    "        else:\n",
    "            if new_col in possible_ref_col_to_melt:\n",
    "                old_col_lst = []\n",
    "                for col, _, _ in new_col_to_old_col_lst[new_col]:\n",
    "                    old_col_lst.append(col)\n",
    "                    if df_with_ref_col is None:\n",
    "                        df_with_ref_col = df[[col]]\n",
    "                    else:\n",
    "                        df_with_ref_col[col] = df[col]\n",
    "                new_col_to_old_col_lst_to_melt[new_col] = old_col_lst.copy()\n",
    "            else:\n",
    "                # make multiple copies with notes\n",
    "                for inx, (col, _, _) in enumerate(new_col_to_old_col_lst[new_col]):\n",
    "                    if df_with_ref_col is None:\n",
    "                        df_with_ref_col = df[[col]]\n",
    "                        df_with_ref_col = df_with_ref_col.rename({col: f\"{new_col}_{inx + 1}\"}, axis = 1)\n",
    "                    else:\n",
    "                        df_with_ref_col[f\"{new_col}_{inx + 1}\"] = df[col]\n",
    "                    df_with_ref_col[f\"{new_col}_{inx + 1} notes\"] = col\n",
    "                    # add these cols in group but not need to melt\n",
    "                    new_col_to_not_melt.append(f\"{new_col}_{inx + 1}\")\n",
    "                    new_col_to_not_melt.append(f\"{new_col}_{inx + 1} notes\")\n",
    "    # Melting stage\n",
    "    if len(new_col_to_old_col_lst_to_melt) > 0:\n",
    "        # now melting column in same groups\n",
    "        # Instead of keep melting, for each group, we make a new dataset of \n",
    "        # [stable col] + [to be melt col] => melt them as a new df\n",
    "        # do this for each gorup and then join together based on stable col\n",
    "        df_with_melt_col = None \n",
    "        for new_col in new_col_to_old_col_lst_to_melt:\n",
    "            temp_df = deepcopy(df_with_ref_col[new_col_to_not_melt + new_col_to_old_col_lst_to_melt[new_col]])\n",
    "            # create a temp row id for stable join\n",
    "            temp_df[\"_row_id\"] = np.arange(temp_df.shape[0])\n",
    "            temp_df = temp_df.melt(\n",
    "                id_vars = new_col_to_not_melt + [\"_row_id\"],    \n",
    "                value_vars = new_col_to_old_col_lst_to_melt[new_col], \n",
    "                var_name = f\"{new_col} notes\",      \n",
    "                value_name = f\"{new_col}\"\n",
    "            )\n",
    "            if df_with_melt_col is None:\n",
    "                df_with_melt_col = deepcopy(temp_df)\n",
    "            else:\n",
    "                df_with_melt_col = df_with_melt_col.merge(temp_df, how = \"inner\", on = [\"_row_id\"] + new_col_to_not_melt)\n",
    "        df_with_melt_col = df_with_melt_col.drop(\"_row_id\", axis = 1)\n",
    "        return df_with_melt_col\n",
    "    else:\n",
    "        return df_with_ref_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e26f716",
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_df_all = None\n",
    "for file in os.listdir(\"./tables\"):\n",
    "    if \".csv\" in file and \"table\" in file and \"harmonized\" not in file:\n",
    "        df = pd.read_csv(f\"./tables/{file}\")\n",
    "        df.columns = ['' if 'Unnamed:' in col else col for col in df.columns]\n",
    "        modified_df = format_original_table(df, remove_unique_col = True)\n",
    "        modified_df[\"file_name\"] = file\n",
    "        if modified_df_all is None:\n",
    "            modified_df_all = modified_df.copy()\n",
    "        else:\n",
    "            modified_df_all = pd.concat([modified_df_all, modified_df], ignore_index = True)\n",
    "        modified_df.to_csv(f\"./harmonized_tables/{file.replace('.csv', '')}_harmonized.csv\", index = False)\n",
    "modified_df_all.to_csv(\"./harmonized_tables/harmonized_table.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8f8b9e",
   "metadata": {},
   "source": [
    "Testing for table harmonization from table_link_to_excel code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e66317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to extract a set of id from table\n",
    "def extract_table_id_lst_from_pmc(pmcid):\n",
    "    url = f\"https://www.ncbi.nlm.nih.gov/research/bionlp/RESTful/pmcoa.cgi/BioC_json/{pmcid}/unicode\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    tables_id_set = set()\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        for d in data:\n",
    "            doc = d[\"documents\"]\n",
    "            for p in doc:\n",
    "                passage = p[\"passages\"]\n",
    "                for item in passage:\n",
    "                    if \"table\" in item.get(\"infons\", \"\").get(\"type\", \"\").lower() and \"id\" in item.get(\"infons\", \"\"):\n",
    "                        tables_id_set.add(item.get(\"infons\", \"\").get(\"id\", \"\"))\n",
    "                        \n",
    "        print(f\"Successfully retrieve list of tables' id\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve list of tables' id {response.status_code}\")\n",
    "        \n",
    "    return [id for id in list(tables_id_set) if id != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65d539c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_valid_snp(row):\n",
    "    row_snp_pattern = r\"\\b(?:rs|s)\\d+\\S*\"\n",
    "    return any(re.search(row_snp_pattern, str(value)) for value in row)\n",
    "\n",
    "def contains_valid_pvalue(row):\n",
    "    row_pvalue_pattern = r\"\\d+\\.\\d+\"\n",
    "    return any(re.search(row_pvalue_pattern, str(value)) for value in row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf97d47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_link_to_excel(pmid, pmcid):\n",
    "    # extract list of table id directly\n",
    "    table_id_list = extract_table_id_lst_from_pmc(pmcid) \n",
    "    # print(table_id_list)\n",
    "\n",
    "    # possible_table_id = [\"T\", \"Tab\", \"Table\", \"Tbl\", \"t\", \"tab\", \"table\", \"tbl\"]\n",
    "\n",
    "    # for i in range(1, num_tables + 1):\n",
    "    for table_id in table_id_list:\n",
    "        table_name = f'tables/{pmid}_{pmcid}_{table_id}_from_pmc.xlsx'\n",
    "        found_table = False\n",
    "        # for pti in possible_table_id:\n",
    "        try:\n",
    "            if BeautifulSoup is None:\n",
    "                raise RuntimeError(\"beautifulsoup4 is not installed. Run: python3 -m pip install beautifulsoup4\")\n",
    "\n",
    "            # pmcid, url_table_id = _extract_pmc_info(f\"https://pmc.ncbi.nlm.nih.gov/articles/{pmcid}\")\n",
    "            # resolved_table_id = table_id\n",
    "\n",
    "            # Fast-path fallback for PMC direct table assets (often bypasses page-level 403).\n",
    "            if pmcid and table_id:\n",
    "                direct_tables = _try_pmc_direct_table_download(pmcid, table_id)\n",
    "                if direct_tables:\n",
    "                    with pd.ExcelWriter(table_name, engine=\"openpyxl\") as writer:\n",
    "                        for i, df in enumerate(direct_tables, start=1):\n",
    "                            df2 = _flatten_columns(df)\n",
    "                            df2.to_excel(writer, index=False, sheet_name=sanitize_sheet_name(\"\", f\"table_{i}\"))\n",
    "                            found_table = True\n",
    "                \n",
    "            if not found_table:\n",
    "                html = \"\"\n",
    "                soup = None\n",
    "                parse_with_xml = False\n",
    "                try:\n",
    "                    html = fetch_html(f\"https://pmc.ncbi.nlm.nih.gov/articles/{pmcid}\")\n",
    "                    soup = BeautifulSoup(html, \"lxml\")\n",
    "                except Exception as e:\n",
    "                    # PMC pages may return 403 to scripts; fallback to Europe PMC XML.\n",
    "                    if pmcid:\n",
    "                        xml = fetch_pmc_fulltext_xml(pmcid)\n",
    "                        soup = BeautifulSoup(xml, \"xml\")\n",
    "                        parse_with_xml = True\n",
    "                    else:\n",
    "                        raise e\n",
    "\n",
    "                if table_id:\n",
    "                    tables = pick_table(soup, table_id=table_id, table_selector=None, table_index=0)\n",
    "                # elif args.all_tables:\n",
    "                #     if parse_with_xml:\n",
    "                #         wraps = soup.find_all(\"table-wrap\")\n",
    "                #         tables = [w.find(\"table\") for w in wraps if w.find(\"table\") is not None]\n",
    "                #     else:\n",
    "                #         tables = soup.select(args.table_selector) if args.table_selector else soup.find_all(\"table\")\n",
    "                #     if not tables:\n",
    "                #         raise ValueError(\"No matched tables found.\")\n",
    "                # else:\n",
    "                #     tables = pick_table(\n",
    "                #         soup,\n",
    "                #         table_id=None,\n",
    "                #         table_selector=args.table_selector,\n",
    "                #         table_index=args.table_index,\n",
    "                #     )\n",
    "                with pd.ExcelWriter(table_name, engine=\"openpyxl\") as writer:\n",
    "                    for i, table in enumerate(tables, start=1):\n",
    "                        df = table_to_dataframe(table)\n",
    "                        caption = \"\"\n",
    "                        cap = table.find(\"caption\")\n",
    "                        if cap:\n",
    "                            caption = _clean_text(cap.get_text(\" \", strip=True))\n",
    "                        sheet = sanitize_sheet_name(caption, fallback=f\"table_{i}\")\n",
    "                        df2 = _flatten_columns(df)\n",
    "                        df2.to_excel(writer, index=False, header=False, sheet_name=sheet)\n",
    "                        found_table = True\n",
    "        \n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        if found_table:\n",
    "            # filter table\n",
    "            df = pd.read_excel(table_name)\n",
    "            df[\"valid_row\"] = df.apply(lambda x: contains_valid_snp(x) and contains_valid_pvalue(x), axis=1)\n",
    "            df = df[df[\"valid_row\"]].drop(\"valid_row\", axis=1).reset_index().drop(\"index\", axis = 1)\n",
    "            if df.shape[0] > 0:\n",
    "                with pd.ExcelWriter(table_name, engine=\"openpyxl\") as writer:\n",
    "                    df.to_excel(writer, index=False)\n",
    "\n",
    "        else:\n",
    "            # delete file\n",
    "            os.remove(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38366afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# Now try to map the columns with the actual col in advp\n",
    "referencing_col_df = pd.read_csv(\"Rules for harmonizing ADVP papers - Main cols.csv\")\n",
    "gwas_column_matching_engine = GWASColumnMatchingEngine(referencing_col_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b85afcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully retrieve list of tables' id\n",
      "Success in extracting from 30448613-PMC6331247\n",
      "Success in harmonizing from 30448613-PMC6331247\n",
      "Successfully retrieve list of tables' id\n",
      "Success in extracting from 30979435-PMC6783343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5b/jn4p_4vx7j9gx2jdrmbjlxnr0000gn/T/ipykernel_5345/2412568355.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  harmonized_df_all = pd.concat([harmonized_df_all, harmonized_df], ignore_index = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success in harmonizing from 30979435-PMC6783343\n",
      "Error in extracting from 31055733-PMC6544706 with error Expecting value: line 1 column 2 (char 1)\n",
      "Success in harmonizing from 31055733-PMC6544706\n",
      "Successfully retrieve list of tables' id\n",
      "Success in extracting from 30617256-PMC6836675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5b/jn4p_4vx7j9gx2jdrmbjlxnr0000gn/T/ipykernel_5345/2412568355.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  harmonized_df_all = pd.concat([harmonized_df_all, harmonized_df], ignore_index = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success in harmonizing from 30617256-PMC6836675\n",
      "Successfully retrieve list of tables' id\n",
      "Success in extracting from 30820047-PMC6463297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justpqa/advpai/gwas_table_extraction.py:560: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_with_ref_col[col] = df[col]\n",
      "/Users/justpqa/advpai/gwas_table_extraction.py:560: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_with_ref_col[col] = df[col]\n",
      "/Users/justpqa/advpai/gwas_table_extraction.py:560: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_with_ref_col[col] = df[col]\n",
      "/Users/justpqa/advpai/gwas_table_extraction.py:539: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_with_ref_col[new_col] = df[new_col_to_old_col_lst[new_col][0][0]]\n",
      "/var/folders/5b/jn4p_4vx7j9gx2jdrmbjlxnr0000gn/T/ipykernel_5345/2412568355.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  harmonized_df_all = pd.concat([harmonized_df_all, harmonized_df], ignore_index = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success in harmonizing from 30820047-PMC6463297\n",
      "Successfully retrieve list of tables' id\n",
      "Success in extracting from 29458411-PMC5819208\n",
      "Success in harmonizing from 29458411-PMC5819208\n",
      "Successfully retrieve list of tables' id\n",
      "Success in extracting from 29777097-PMC5959890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5b/jn4p_4vx7j9gx2jdrmbjlxnr0000gn/T/ipykernel_5345/2412568355.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  harmonized_df_all = pd.concat([harmonized_df_all, harmonized_df], ignore_index = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success in harmonizing from 29777097-PMC5959890\n",
      "Successfully retrieve list of tables' id\n",
      "Success in extracting from 30651383-PMC6369905\n",
      "Success in harmonizing from 30651383-PMC6369905\n",
      "Error in extracting from 31497858-PMC6736148 with error Expecting value: line 1 column 2 (char 1)\n",
      "Success in harmonizing from 31497858-PMC6736148\n",
      "Successfully retrieve list of tables' id\n",
      "Success in extracting from 30930738-PMC6425305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5b/jn4p_4vx7j9gx2jdrmbjlxnr0000gn/T/ipykernel_5345/2412568355.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  harmonized_df_all = pd.concat([harmonized_df_all, harmonized_df], ignore_index = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success in harmonizing from 30930738-PMC6425305\n",
      "Successfully retrieve list of tables' id\n",
      "Success in extracting from 31426376-PMC6723529\n",
      "Success in harmonizing from 31426376-PMC6723529\n",
      "Successfully retrieve list of tables' id\n",
      "Success in extracting from 29967939-PMC6280657\n",
      "Success in harmonizing from 29967939-PMC6280657\n",
      "Successfully retrieve list of tables' id\n",
      "Success in extracting from 29107063-PMC5920782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5b/jn4p_4vx7j9gx2jdrmbjlxnr0000gn/T/ipykernel_5345/2412568355.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  harmonized_df_all = pd.concat([harmonized_df_all, harmonized_df], ignore_index = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success in harmonizing from 29107063-PMC5920782\n",
      "Successfully retrieve list of tables' id\n",
      "Success in extracting from 29274321-PMC5938137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5b/jn4p_4vx7j9gx2jdrmbjlxnr0000gn/T/ipykernel_5345/2412568355.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  harmonized_df_all = pd.concat([harmonized_df_all, harmonized_df], ignore_index = True)\n",
      "/Users/justpqa/advpai/gwas_table_extraction.py:560: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_with_ref_col[col] = df[col]\n",
      "/Users/justpqa/advpai/gwas_table_extraction.py:560: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_with_ref_col[col] = df[col]\n",
      "/Users/justpqa/advpai/gwas_table_extraction.py:539: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_with_ref_col[new_col] = df[new_col_to_old_col_lst[new_col][0][0]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success in harmonizing from 29274321-PMC5938137\n",
      "Successfully retrieve list of tables' id\n",
      "Success in extracting from 30413934-PMC6358498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5b/jn4p_4vx7j9gx2jdrmbjlxnr0000gn/T/ipykernel_5345/2412568355.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  harmonized_df_all = pd.concat([harmonized_df_all, harmonized_df], ignore_index = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success in harmonizing from 30413934-PMC6358498\n",
      "Successfully retrieve list of tables' id\n",
      "Success in extracting from 30805717-PMC7193309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/justpqa/advpai/gwas_table_extraction.py:558: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_with_ref_col[col + \" \"] = df[col]\n",
      "/Users/justpqa/advpai/gwas_table_extraction.py:539: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_with_ref_col[new_col] = df[new_col_to_old_col_lst[new_col][0][0]]\n",
      "/var/folders/5b/jn4p_4vx7j9gx2jdrmbjlxnr0000gn/T/ipykernel_5345/2412568355.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  harmonized_df_all = pd.concat([harmonized_df_all, harmonized_df], ignore_index = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success in harmonizing from 30805717-PMC7193309\n",
      "Successfully retrieve list of tables' id\n",
      "Success in extracting from 30636644-PMC6330399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5b/jn4p_4vx7j9gx2jdrmbjlxnr0000gn/T/ipykernel_5345/2412568355.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  harmonized_df_all = pd.concat([harmonized_df_all, harmonized_df], ignore_index = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success in harmonizing from 30636644-PMC6330399\n",
      "Successfully retrieve list of tables' id\n",
      "Success in extracting from 29752348-PMC5976227\n",
      "Success in harmonizing from 29752348-PMC5976227\n",
      "Successfully retrieve list of tables' id\n",
      "Success in extracting from 28560309-PMC5440281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5b/jn4p_4vx7j9gx2jdrmbjlxnr0000gn/T/ipykernel_5345/2412568355.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  harmonized_df_all = pd.concat([harmonized_df_all, harmonized_df], ignore_index = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success in harmonizing from 28560309-PMC5440281\n",
      "Successfully retrieve list of tables' id\n",
      "Success in extracting from 27899424-PMC5237405\n",
      "Success in harmonizing from 27899424-PMC5237405\n"
     ]
    }
   ],
   "source": [
    "test_papers_info = [\n",
    "    (30448613, \"PMC6331247\"), (30979435, \"PMC6783343\"), (31055733, \"PMC6544706\"),  (30617256, \"PMC6836675\"),\n",
    "    (30820047, \"PMC6463297\"), (29458411, \"PMC5819208\"), (29777097, \"PMC5959890\"), (30651383, \"PMC6369905\"),\n",
    "    (31497858, \"PMC6736148\"), (30930738, \"PMC6425305\"), (31426376, \"PMC6723529\"), (29967939, \"PMC6280657\"),\n",
    "    (29107063, \"PMC5920782\"), (29274321, \"PMC5938137\"), (30413934, \"PMC6358498\"), (30805717, \"PMC7193309\"),\n",
    "    (30636644, \"PMC6330399\"), (29752348, \"PMC5976227\"), (28560309, \"PMC5440281\"), (27899424, \"PMC5237405\"),\n",
    "]\n",
    "\n",
    "referencing_col_df = pd.read_csv(\"Rules for harmonizing ADVP papers - Main cols.csv\")\n",
    "gwas_column_matching_engine = GWASColumnMatchingEngine(referencing_col_df)\n",
    "\n",
    "file_name_to_matching = {}\n",
    "for pmid, pmcid in test_papers_info:\n",
    "    try:\n",
    "        table_link_to_excel(pmid, pmcid)\n",
    "        print(f\"Success in extracting from {pmid}-{pmcid}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in extracting from {pmid}-{pmcid} with error {e}\")\n",
    "\n",
    "    try:\n",
    "        harmonized_df_all = pd.DataFrame(columns = referencing_col_df[\"column\"].to_list())\n",
    "        for file_name in os.listdir(\"tables\"):\n",
    "            if str(pmid) in file_name and pmcid in file_name:\n",
    "                df = pd.read_excel(f\"tables/{file_name}\")\n",
    "\n",
    "                # save matching dict for debug\n",
    "                file_name_to_matching[file_name] = gwas_column_matching_engine.match_many_col_to_ref_col(df)\n",
    "\n",
    "                harmonized_df = format_original_table(df, gwas_column_matching_engine, remove_unique_col = True)\n",
    "                if harmonized_df_all is None:\n",
    "                    harmonized_df_all = harmonized_df.copy()\n",
    "                else:\n",
    "                    harmonized_df_all = pd.concat([harmonized_df_all, harmonized_df], ignore_index = True)\n",
    "        for col in referencing_col_df[\"column\"].unique():\n",
    "            if col not in harmonized_df_all.columns:\n",
    "                harmonized_df_all[col] = pd.NA\n",
    "        harmonized_df_all.to_csv(f\"pred_tables/{pmid}_{pmcid}.csv\", index = False)\n",
    "        print(f\"Success in harmonizing from {pmid}-{pmcid}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in harmonizing from {pmid}-{pmcid} with error {e}\")\n",
    "with open(\"test_matching_dict.json\", \"w\") as f:\n",
    "    json.dump(file_name_to_matching, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
